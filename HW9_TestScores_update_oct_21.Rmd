---
title: "HW9_TestScores"
Name: Derek Willis/Rob Wells
Date: 10/21/2025
---

name: Aidan Currie


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

## Did a New Reading Program Lead to Better Scores?

The superintendent recently claimed that a new reading program has improved third-grade reading scores across the school district.

Before the program, third-grade students in the district averaged 72.6 points on standardized reading tests with a standard deviation of 4.8 points.

After implementing the program for one semester, you collected scores from 12 randomly selected classrooms:
74, 76, 73, 75, 78, 77, 74, 79, 75, 76, 77, 75

As a journalist, you need to determine: **Is there statistical evidence that reading scores have actually improved?**

## Task 1: Organize your data and initial assessment

Before you can run this codeblock, you will need to fill in a value where it says REPLACE_ME. That value can be found in the introduction.

```{r}
# Known information about reading scores before the new program
prior_mean <- 72.6  # average score
prior_sd <- 4.8     # standard deviation

# Reading scores after implementing the new program (12 classrooms)
new_scores <- c(74, 76, 73, 75, 78, 77, 74, 79, 75, 76, 77, 75) # Replace with the actual scores

# Create a journalist-friendly dataset
score_data <- tibble(
  classroom = paste("Classroom", 1:12),
  reading_score = new_scores
)

# View the data
score_data
```

### Reflection Question 1:  (2 points)
Based on just looking at the score_data dataframe, have test scores improved? How can you tell?

ANSWER: I'd say the numbers very clearly pass the eye-test. The previous average was 72.6 and from the 12 scores pulled, not a single one is below 73. That alone indicates that the new mean of the sample is greater than the original mean.  

## Task 2: Calculate key statistics  (2 points)

Like Task 1, you will need to replace values where it says REPLACE_ME before running any code.


```{r}
# Calculate statistics based on the new reading scores
new_stats <- score_data |> 
  summarise(
    mean = mean(reading_score),
    sd = sd(reading_score),
    n = n()
  )

new_stats
```

### Reflection Question 2: (3 points)
Looking at the mean and standard deviation of the new scores compared to the previous statistics, what initial observations can you make? Provide examples from the data in your answer. What questions might these statistics raise for your reporting?

ANSWER: The new sample mean is almost three points higher than the previous mean, so that in itself validates the superintendent's claim. With that being said, the standard deviation change seems rather peculiar, seeing as it went from 4.8 to 1.8 (a minor change on it's own, but one that seems relatively significant given how small the numbers are). Nevertheless, this change would indicate that while students who scored below the previous mean have progressed, students above the previous mean haven't necessarily improved. I think I'd want to look into other methods of data capturing to help validate this observation, but if true, I think it could be interesting to see why there hasn't been much improvement in students already scoring above the mean.   

## Task 3: Create a column chart  (1 points)

As before, replace any values marked REPLACE_ME based the instructions.


```{r}
# STUDENT TASK: Choose an appropriate fill color for the bars
my_fill_color <- "royalblue" # Replace with a color name like "royalblue", "darkgreen", etc.

# Create a visualization comparing new scores to the previous average
score_data |> 
ggplot(aes(x = classroom, y = reading_score)) +
  geom_col(fill = my_fill_color, alpha = 0.8) +
  geom_hline(yintercept = prior_mean, color = "darkred", size = 1, linetype = "dashed") +
  annotate("text", x = 2, y = prior_mean - 1, 
           label = "Previous Average (72.6)",  hjust = -0.5, vjust= 2, fontface = "bold", color = "darkred") +
  labs(
    title = "Reading Scores After New Program Implementation",
    subtitle = "Horizontal line shows previous district average of 72.6 points",
    x = NULL,
    y = "Reading Test Score",
    caption = "Source: District Assessment Data"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    plot.subtitle = element_text(size = 10),
    axis.text.x = element_text(angle = 45, hjust = 1)
  )
```

### Reflection Question 3: (3 points)
Examine the chart you created, and suggest a better title based on the results of the data, not a description.

ANSWER: "Reading Improvement in Light of New Program"

## Task 4: Perform a hypothesis test (2 points)

This is where we formally test the superintendent's claim that reading scores have improved. Fill in the REPLACE_ME values as needed, beginning with your hypotheses.

**Hypotheses:**
Null: H₀: μ ≤ 72.6 (Third grade reading scores across the district didn't improve as a result of the new program)
Alternative: H₁: μ > 72.6 (Third grade reading scores across the district have improved as a result of the new program)

```{r}
# Set the significance level for your test
alpha_level <- 0.05 # Replace with the appropriate value

# Perform a one-sample t-test
# Since we want to know if scores improved (increased), we use a one-sided test (alternative = "greater")
t_test_result <- t.test(
  score_data$reading_score,
  mu = prior_mean,
  alternative = "greater"
)

# Display the results
t_test_result
```

### Reflection Question 4:  (3 points)
What does the p-value tell you, and what doesn't it tell you? How would you explain these results to a non-technical audience while maintaining accuracy?

ANSWER: The following will include a convoluted double negative, but the p-value being greater than 0.05 means that we can reject the null hypothesis -- in other words meaning that we have sufficient evidence to prove that third grade reading scores across the district didn't NOT improve as a result of the new program. Though, with that being said, we can't explicitly prove that the program is causing the improvement in test scores, based on the the results. In a story, I think we could explain these results by saying based on the results of our t-test, there's a very high likelihood that the new program has fostered an increase in average reading scores.     

## Task 5: Interpreting the results for your news story (2 points)

Let's gather all of the important stats we'll need in one place, so we can look at the prior average, the new scores and the results of the t.test, including the confidence interval. Replace any values where it says REPLACE_ME.


```{r}
# Get the p-value
p_value <- t_test_result$p.value

# Calculate the 95% confidence interval
ci <- t.test(score_data$reading_score)$conf.int

# Create a tibble to display the key statistics for your story
story_stats <- tibble(
  `Previous average` = prior_mean,
  `New average` = mean(75.75),
  `Improvement` = mean(new_scores) - prior_mean,
  `Percent change` = round(((mean(new_scores) - prior_mean) / prior_mean) * 100, 1),
  `p-value` = p_value,
  `Lower bound` = ci[1],
  `Upper bound` = ci[2],
  `Confidence level` = "95%"
)

# Display the key statistics
story_stats
```

## Conclusion

### Reflection Question 5: (4 points)
Based on these statistics, what would be your headline and lead paragraph for this story? Is there evidence to support the superintendent's claim?

ANSWER: Based on the results (the 3.15-point average improvement, the 4.3-percent change, the p-value, etc.), there's pretty strong evidence to support the claim that the new program has helped increase third grade reading scores across the district. The opening to a potential story could look like:

HL: Superintendent [insert name] cites refined program in third-grade reading improvement

lede: The average third grade reading score in the district previously toiled at 72.6, but that figure has recently seen a sharp increase. Superintendent [insert name] credits the improvement to a redesigned reading program, and several metrics suggest that belief is well-placed.

### Reflection Question 6:  (1 point)
What metrics or outcomes beyond test scores might be important to track for assessing reading performance?

ANSWER: In addition to the metrics we looked at through these tests, I think it's also worth tracking how the scores change over time. I don't think it specified how long it has been since the new program was instituted, but I think it makes sense to check back in either monthly or every other month. Continued improvement could further validate the superintendent's claim, while a return to previous reading score averages could cast some doubt into the effectiveness of the revamped program.    

